%\documentclass{beamer}
\documentclass[handout]{beamer}

\usepackage{tikz,fancyvrb,hyperref,multicol,relsize,verbatim,ulem}%,enumitem}
%\usepackage{enumitem}
\usepackage{mathrsfs,mathtools,framed}
\usepackage[T1]{fontenc}


\usetikzlibrary{shapes,arrows}


%\usetheme{Warsaw}


%\usecolortheme{beaver}


\usefonttheme{professionalfonts}
%\setbeamercolor{block}{bg=red, fg=white}
\setbeamercolor{block title}{bg=blue!25}
\setbeamercolor{block body}{bg=blue!15}


\AtBeginSection[ ] {
\begin{frame}<beamer>{ }
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame} }



\newcommand{\leqAS}{\overset{\textrm{a.s.}}{\leq}}

\renewcommand{\b}{\textbf}
\newcommand{\bX}{X}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\hJ}{\widehat{J}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\hcQ}{\widehat{\mathcal{Q}}}
\renewcommand{\bar}{\overline}
\newcommand{\proj}{\mathcal{P}}
\newcommand{\cY}{\mathcal{Y}}
%\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\pow}{\textnormal{Pow}}


\newcommand{\bcol}[2]{{\usebeamercolor[fg]{#1} #2}}
\newcommand{\bstrc}[1]{\bcol{structure}{#1}}
\newcommand{\balrt}[1]{\bcol{alerted text}{#1}}

%\usepackage{multimedia}
\usepackage{../../mycommands}
\newcommand{\hk}{\hat{k}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\Err}{\cE}

\newcommand*\mystrut{\vrule width0pt height0pt depth1.5ex\relax}
\newcommand{\underlabel}{\underbracket[1pt][.5pt]{\mystrut \quad\;\; \sub \quad\;\; }}


\title{Sequential Adaptive Model Selection}

\author{Will Fithian\\~\\
  {\small Joint with:}\\
  Jonathan Taylor,\\ Rob Tibshirani,\\ Ryan Tibshirani}
\date{\today}


\begin{document}


\frame{\titlepage}


\section{Sequential Model Selection}


\frame{\frametitle{Motivation: Model Selection in Regression}
  \bstrc{Given:} response $Y \in \R^n$, predictors $X_1,\ldots, X_p$
  \vskip+1em
  Run $d$ steps of LARS / LASSO / Forward Stepwise: 
  \vskip+1em
  $\quad\imp$ sequence of selected \bstrc{variables}
  $j_1,\ldots, j_d \in [p]=\{1,\ldots,p\}$
  \vskip+1em
  $\quad\imp$ sequence of \bstrc{active sets} 
  $E_k = \{j_1,\ldots,j_k\}$
  \[\emptyset \sub E_1 \sub \cdots \sub E_d\]
  $\quad\imp$ sequence of nested \bstrc{regression models} 
  \[
  M_0 \sub M_1 \sub \cdots \sub M_d
  \]
  \[
  M_k:\;Y \sim \cN_n(X_{E_k}\beta, \sigma^2 I_n)
  \]
  \bstrc{Goal:} choose $k$
}

\frame{\frametitle{Motivation: Principal Components Analysis}
  \bstrc{Given:} data matrix $X \in \R^{n\times m}$, sample covariance
  \[
  S = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2
  \]
  Run $d$ steps of PCA:
  \vskip+1em
  $\quad\imp$ sequence of selected \bstrc{eigenvectors}
  $u_1, \ldots, u_d$
  \vskip+1em
  $\quad\imp$ sequence of nested \bstrc{Wishart models}
  \[
  M_0 \sub M_1 \sub \cdots \sub M_d
  \]
  \[
  M_k:\; (n-1)S \sim W_m
  \left(\lambda_0 I_m + \sum_{i=1}^k\lambda_i u_iu_i', 
    \;\;\;n-1\right)
  \]
  \bstrc{Goal:} choose $k$
}

\frame{\frametitle{Motivation: Exploratory Model Selection}
  \bstrc{General Setup:} Data $Y\sim F$, $F$ unknown
  \vskip+1em
  Use $Y$ to \bstrc{adaptively} generate sequence of nested models
  \[
  M_0(Y) \sub M_1(Y) \sub \cdots \sub M_d(Y)
  \]
  \bstrc{Goal 1:} for $1 \leq k \leq d$, construct $p_k(Y)$ to test 
  \[
  H_{0,k}:\; F \in M_{k-1}
  \quad \text{ vs. } \quad
  H_{1,k}:\; F \in M_k \setminus M_{k-1},
  \]
  adjusting for selection
  \vskip+1em
  \bstrc{Goal 2:} choose smallest adequate model $M_{k_0}$
}

\frame{\frametitle{Which Null?}
  Regression case: Should we test \bstrc{selected model} null
  \[
  H_{0,k}:\; F\in M_{k-1} \iff 
  Y \sim \cN(X_{E_{k-1}}\beta, \;\;\sigma^2I_n)
  \]
  or \bstrc{full model} null
  \[
  \widetilde H_{0,k}:\; Y \sim 
  \cN(X_{[p]\setminus j_k}\beta, \;\;\sigma^2I_n)
  \]
  Suppose $X_1$ signal, $X_2,\ldots,X_p$ noise, $X_1$ selected at step $k=10$
  \vskip+1em
  We take \bstrc{model-centric} point of view: \\
  models $M_1,\ldots,M_9$ are incorrect
  $\imp$ want to select $M_{10}$
  \vskip+1em
  Contrast with \bstrc{variable-centric} point of view 
  (e.g., knockoffs):\\ 
  variables $j_1,\ldots,j_9$ are false discoveries
  \vskip+1em 
  Will revisit this question later...
}

\section{Inference for One Step}

\frame{\frametitle{Inference for Step $k$}
  At step $k$, construct valid $\phi_k(Y)$ to test
  \[
  H_{0,k}:\; F \in M_{k-1}(Y)
  \quad \text{ vs. } \quad
  H_{1,k}:\; F \in M_k(Y) \setminus M_{k-1}(Y),
  \]
  Want selective test $\phi_k(Y)$ for \bstrc{random question}
  \[
  Q_k(Y) = (M_{k-1}(Y), M_k(Y))
  \]
  \vskip+1em
  Game plan:
  \begin{enumerate}
  \item produce $\phi_{k,q}(y)$ for each fixed $q = (m_0, m_1)$
  \item combine to get test of $Q_k(Y)$:
    \[
    \phi_k(y) = \phi_{k,Q_k(y)}(y)    
    \]
  \item convert tests to $p$-values
  \end{enumerate}
}


\frame{\frametitle{Fixed-Question Analysis}
  Consider fixed candidate $q=(m_0,m_1)$ for $Q_k=(M_{k-1},M_k)$
  \vskip+1em
  Selection event:
  \[
  A_{k,q} = \{M_{k-1}(Y) = m_0, M_k(Y) = m_1\}
  \]
  FST (2014): \bstrc{selective level-$\alpha$ test} $\phi_{k,q}(y)$ controls
  \[
  \E_F\left[ \phi_{k,q}(Y) \mid Q_k(Y) = q\right] \leq \alpha, 
  \quad \forall F \in m_0(q)  
  \]
  Doable in regression models (nested exp. fam. models)
}

\frame{\frametitle{Fixed-Question Analysis: Regression Case}
  Define regression model with active set $E \sub p$
  \[
  M(E) \triangleq \left\{\cN_n\left(X_E\beta,\; \sigma^2I\right):\;
    \beta\in \R^{|E|}, \sigma^2>0\right\}
  \]
  Assume $|E_k(Y)| \eqAS k$. 
  $q$ for testing $H_0:\; \beta_j = 0$ in $M(E)$:
  \note{assume method adds variable to active set at each step}
  \[
  q(E,j) \triangleq (M(E\setminus j), M(E)), \;\;\; j \in E \sub [p]
  \]
  Selection events for $k=|E|$
  \[
  A(E,j) \triangleq A_{q(E,j)} = \{E_k(Y) = E, j_k(Y) = j\}
  \]
  Forward Stepwise, LARS, LASSO: $A(E,j)$ is union of polytopes
}

\begin{comment}
\frame{\frametitle{}
  Selected-model analysis:
  \[
  Y \sim \exp\left\{\frac{1}{\sigma^2} y'X_{E_k}\beta
    - \frac{1}{2\sigma^2}\|y\|^2\right\}\; 1\{y \in A_{k,q}\}
  \]
  Condition on ${X'}_{E\setminus j}y$:
  \[
  \L\left(Y \mid {X'}_{E\setminus j}Y, A_q\right)
  \]
  Tests for regression based on 
  \[
  \L_{\beta}( X_j'Y \gv)
  \]
}

\frame{\frametitle{f}
$\phi_{k,q}$ to for test of random $Q_k(Y)$: 
    $\phi_k(y) = \phi_{k,Q_k(y)}(y)$
    \begin{align*}
      \phi_k(y) &= \phi_{k,Q_k(y)}(y) \\
      \Longrightarrow 
      &\E_F\left[\phi_k(Y) \mid M_{k-1}(Y), M_k(Y)\right]
      1\{F \in M_{k-1}(Y)\} \leq \alpha, \quad \forall F\\
      \Longrightarrow 
      &\E_F\left[\phi_k(Y) \mid 1\{F \in M_{k-1}(Y)\} \right]
       \leq \alpha, \quad \forall F
    \end{align*}
    \note{note that $Q_k$ is random, not $F$}
  \end{enumerate}
  First, 
  \[
  q
  \]    
}

\frame{\frametitle{Review: Post-Selection Inference}
    \[
    \E_F\left[ \phi_{k,q}(Y) \mid Q_k(Y) = q\right] \leq \alpha, 
    \quad \forall F \in m_0(q)
    \]
  
}
\end{comment}

\frame{\frametitle{Building Blocks}
  Test for a \bstrc{random} question $q_k(Y) = (M_{k-1}(Y), M_k(Y))$
  \vskip+1em
  For fixed candidate \bstrc{question} $q=(H_0,M)$, \bstrc{selection event} is
  \[
  A_{k,q} = \left\{ y:\; Q_k(Y) = q \right\}
  \]
  Methods in Fithian, Sun, \& Taylor $\imp$ \bstrc{selective $p$-value}
  \[
  \L_F\left(p_{k,q}(Y) \mid Y \in A_{k,q}\right) = \text{Unif}[0,1], \quad \forall F \in H_0(q)
  \]
  ``Stitch together'' $p_{k,q}$ to test \bstrc{random} question 
  $q_k(Y) = (M_{k-1}(Y), M_k(Y))$
  \[
  p_k(Y) = p_{k, q_k(Y)}(Y)
  \]
}

\frame{\frametitle{Selected Model Vs Saturated Model}
  
}


\frame{\frametitle{Selection Variables}
  \bstrc{Recall:} sometimes, want to condition on more than $A_q$
  \vskip+1em

}




\section{Sequential Hypothesis Testing}

\frame{\frametitle{Things to Mention}
  ``Simple Stop'' (controls FWER regardless of dependence?)
  \vskip+1em
  Options:\\
  Method $\times$ \{ Regression, Ex.Fam., Generic \}
  $\times$ \{ Saturated, Selected \}
  $\times$ Selection variable
  $\times$ Stopping rule
  $\times$ ???
}

\frame{\frametitle{Stopping Rules: Non-Adaptive Setting}
  Observe $Y \sim F$
  \vskip+1em
  Given \bstrc{fixed} sequence of nested statistical models
  \[
  M_0 \sub M_1 \sub \cdots \sub M_d % \qquad \text{(fixed, for now)}
  \]
  and $p$-values $p_k(Y), k=1,\ldots,d$ 
  \note{stochastically smaller}
  \vskip+1em
  \bstrc{Completion index:} $k_0 = \min \{k:\; F \in M_k\}$ \pause
  (first correct model)
  \vskip+1em
  \bstrc{Stopping rule:} $\hk(Y)$ estimator of $k_0$.
  \vspa
  \bstrc{Goal 2a:} Control type I errors  $(\hk(Y) - k_0)_+$
  \vskip+1em
  \bstrc{Goal 2b:} Minimize type II errors $(k_0 - \hk(Y))_+$
}

\frame{\frametitle{Strong Stop}
  G'Sell, Wager, Chouldechova, \& Tibshirani (2013)
  \[
  \hk_{S}(Y) = \max\left\{k \in \{1,\ldots,m\} :\;
    \exp\left(\sum_{i=k}^m \frac{\log p_i}{i}\right) 
    \leq \frac{\alpha k}{m}\right\}
  \]
  Controls \bstrc{familywise error rate:}
  \[
  \P\left[\hk_S(Y) > k_0\right] \leq \alpha
  \]
  under independence condition:
  \[
  (p_{k_0+1},\ldots,p_d) \mid (p_1,\ldots,p_k) 
  \sim \text{Unif}[0,1]^{d-k_0}
  \]
}



\frame{\frametitle{Forward Stop}
  G'Sell, Wager, Chouldechova, \& Tibshirani (2013)
  \[
  \hk_{F}(Y) = \max\left\{k \in \{1,\ldots,m\} :\;
    -\frac{1}{k}\sum_{i=1}^k \log(1-p_i) \leq \alpha\right\}
  \]
  Controls \bstrc{false discovery rate:}
  \[
  \E\left[\frac{(\hk_F(Y) - k_0)_+}{\hk_F(Y) \vee 1}\right] \leq \alpha
  \]
  under independence condition:
  \[
  (p_{k_0+1},\ldots,p_d) \mid (p_1,\ldots,p_k) 
  \sim \text{Unif}[0,1]^{d-k_0}
  \]
}


\frame{\frametitle{Post-Selection Inference}
  G'Sell et al. implicitly assume $M_k$ fixed for $1 \leq k \leq d$.
  \vskip+1em
  Adaptive variable selection $\imp$ \balrt{random} sequence
  \[
  M_0(Y) \sub M_1(Y) \sub \cdots \sub M_d(Y)
  \]
  Completion index $k_0(Y)$ is \balrt{random} (function of $M(Y)$)
  \vskip+1em
  $\L(p_k(Y))$ is \balrt{random} (uniform if $k > k_0(Y)$, otherwise not)
  \vskip+1em
  \balrt{NB:} $F$ is \balrt{not} random
  \vskip+1em
  What error rate \balrt{should} we control?
}


\frame{\frametitle{Post-Selection Error Rates}
  Error rate = expectation of loss $\Err(\hk(Y), k_0(Y))$
  \[
  \Err(\hk, k_0) = \left\{
    \begin{matrix}
      (\hk - k_0)_+/(\hk \vee 1) & \text{FDR}\\
      1\{\hk > k_0\} & \text{FWER}\\
      \cdots & \cdots
    \end{matrix}
  \right.
  \]
  Strongest adaptive error control guarantee:
  \[
  \E\left[\Err(\hk, k_0) \mid M_0,\ldots,M_d\right] \leq \alpha
  \]
  Implies
  \[
  \E\left[\Err(\hk, k_0) \mid k_0\right] \leq \alpha, 
  \quad \text{ and } \quad
  \E\left[\Err(\hk, k_0)\right] \leq \alpha
  \]
}

\frame{\frametitle{Post-Selection $p$-Values}
  We want \bstrc{strongest} adaptive guarantee
  \[
  \E\left[\Err(\hk, k_0) \mid M_0,\ldots,M_d\right] \leq \alpha
  \]
  for $\hk_F(p_1,\ldots,p_d)$ and $\hk_S(p_1,\ldots,p_d)$.
  \vskip+1em
  \bstrc{Goal:} Given arbitrary selection rule
  \[
  M_0(Y) \sub M_1(Y) \sub \cdots \sub M_d(Y),
  \]
  generate $p_1(Y),\ldots,p_d(Y)$ with 
  \[
  (p_{k_0+1},\ldots,p_d) 
  \mid (M_1, \ldots, M_d, \;k_0, \;p_1, \ldots, p_{k_0}) 
  \sim \text{Unif}[0,1]^{d-k_0}
  \]
  
}

\frame{\frametitle{Relating the two}
  For independent null $p$-values, condition on stuff up to $k-1$
  \begin{block}{Lemma (FTTT 2015)}
    Assume that for all $F,k,\alpha$,
    \[
    \P_F\left(p_k \leq \alpha \mid M_{[k-1]},p_{[k-1]}\right) 
    \1\{F \in M_{k-1}\} \leqAS \alpha.
    \]
    Then, 
    \[
    \P_F\left(p_{k+1}\leq \alpha_{k+1}, \ldots, p_d \leq \alpha_d
      \mid p_{[k]}, k_0(Y)=k\right) 
    \leqAS \prod_{\ell=k+1}^d \alpha_\ell 
    \]
  \end{block}
  Proof:
  \[
  \P_F\left(p_d \leq \alpha_d \mid p_{[d-1]}, k_0(Y)=k\right) 
  \leqAS \alpha_d
  \]
}

\frame{\frametitle{Proof}
  \[
  \P_F(p_d \leq \alpha_d \mid p_{[d-1]}, k_0(Y)=k\right) \leq \alpha_d
  \]
  
  \begin{align*}
    \P_F
    &\left(p_{k+1} \leq \alpha_{k+1}, \;\;\ldots, 
      \;\;p_d \leq \alpha_d
      \mid M_{[k]}, p_{[k]}\right)\1\{F \in M_{k}\}\\
    &= \idotsint\limits_{0\qquad 0}^{\;\alpha_{k+1}\quad \;\alpha_d}
    d\P_F\left(p_{k+1},\ldots, p_d \mid M_{[k]}, p_{[k]}\right)\\
    &= \idotsint\limits_{0\qquad 0}^{\;\alpha_{k+1}\quad \;\alpha_d}
    \,d\P_F\left(p_d \mid p_{[d-1]},M_{[k]}\right)
    \cdots d\P_F\left(p_{k+1} \mid p_{[k]},M_{[k]}\right)\\
    &\leq \alpha_d
    \idotsint\limits_{0\qquad 0}^{\;\alpha_{k+1}\quad \;\alpha_{d-1}}
    \,d\P_F\left(p_{d-1} \mid p_{[d-2]},M_{[k]}\right)
    \cdots d\P_F\left(p_{k+1} \mid p_{[k]},M_{[k]}\right)\\
    &\leq \cdots \leq  \prod_{\ell=k+1}^d \alpha_\ell 
%    &\leq \alpha_d \int_0^{\alpha_{k+1}}\cdots\int_0^{\alpha_{d-1}}
%    \,d\P_F\left(p_{d-1} \mid p_{[d-2]},M_{[k]}\right)
%    \cdots d\P_F\left(p_{k+1} \mid p_{[k]},M_{[k]}\right)
  \end{align*}
  \pause
  Marginalize over $M_{[k]}$ to complete proof \qed
}


\section{Existing Sequential Selection Procedures}


\frame{\frametitle{Covariance Test}
  
}

\frame{\frametitle{Covariance Test}
  
}

\frame{\frametitle{Spacings Test}

}

\frame{\frametitle{Spacings Test}

}


\section{Sequential Selection For Linear Regression}


\frame{\frametitle{Example: Forward Stepwise Linear Regression}
  
}

\frame{\frametitle{Comparison With Knockoffs}
  
}

\section{Sequential Selection For Exponential Families}

\frame{\frametitle{Generalization}

}

\frame{\frametitle{Principal Components Analysis}

}

\section{Simulation: Sparse Linear Regression}

\frame{\frametitle{Generating Model}

}

\frame{\frametitle{Comparisons}

}

\frame{\frametitle{The End}
  \begin{center} 
    {\Huge Thanks!}
  \end{center}
}

\end{document}