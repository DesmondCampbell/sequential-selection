\documentclass{article}


\usepackage[round]{natbib}
\usepackage{amsmath,amssymb,amsthm,bm,enumerate,mathrsfs,mathtools}
\usepackage{latexsym,color,verbatim,multirow}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage[margin=1.3in]{geometry}
\usetikzlibrary{shapes,arrows}
\tikzstyle{block} = [rectangle, draw, fill=white!20,
    text width=7em, text centered, rounded corners, minimum height=4em]
\tikzstyle{title} = [text width=7em, text centered, font=\bfseries]
\tikzstyle{line} = [draw, -latex']


\usepackage{mycommands}

\begin{document}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{model}[theorem]{Model}

\theoremstyle{definition}
\newtheorem{example}{Example}

\newcommand{\cM}{\mathcal{M}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\FDR}{\textnormal{FDR}}
\newcommand{\FCR}{\textnormal{FCR}}
\newcommand{\crt}{\phi}
\newcommand{\M}{\mathcal{M}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\Gv}{\;\;\big|\;\;}
%\newcommand{\cP}{\mathcal{P}}
\newcommand{\proj}{\cP}
\newcommand{\pow}{\text{Pow}}
\newcommand{\sF}{\mathscr{F}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\hJ}{\widehat{J}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\hK}{\widehat{K}}
\newcommand{\leqAS}{\overset{\textrm{a.s.}}{\leq}}


\newcommand*\mystrut{\vrule width0pt height0pt depth1.5ex\relax}
\newcommand{\underlabel}{\underbracket[1pt][.5pt]{\mystrut \quad\;\; \sub \quad\;\; }}
\newcommand{\JTcomment}[1]{{\color{blue}{(JT: \bf \sc #1) }}}
\newcommand{\WFcomment}[1]{{\color{red}{(WF: \bf \sc #1) }}}

\title{Adaptive Sequential Model Selection}
\maketitle

\begin{abstract}
  
\end{abstract}


\begin{example}[Linear Regression Variable Selection]\label{ex:linReg}
  Assume that
  \begin{equation}
    Y \sim N(X\beta, \sigma^2I).
  \end{equation}
  With many of the $\beta_j=0$. Our goal is to find a small subset of variables containing the true model.
\end{example}

\section{The Problem of Sequential Model Selection}

\subsection{Notation and Problem Setting}

Suppose that after proposing a random sequence of nested models 
\begin{equation}
  M_0(Y) \sub M_1(Y) \sub \cdots \sub M_d(Y) \sub \cF,
\end{equation}
we wish to sequentially test each $M_k$ against $M_{k-1}$. Let $\bM(Y)$ denote the sequence $(M_0,\ldots,M_d)$, and define its {\em completion index} $K$ to be the index of the first model containing $F$, i.e.
\begin{equation}
  K(\bM) = \min \{k:\, F\in M_k\}.
\end{equation}
Let $N_k$ denote the event $\{Y\in M_{k-1}\}$, i.e. the event that the null hypothesis for the test at stage $k$ is true. $Y\in N_k$ if and only if $k>K$.

For example, suppose that we use the LARS algorithm to build up a linear regression model. Let $J_k(Y)$ be the index of the $k$th variable to have a nonzero coefficient in the LARS path, so that we add variables to the model in the order $X_{J_1}, X_{J_2}, \ldots, X_{J_d}$. It is natural to take model $M_k(Y)$ as
\begin{equation}
  M_k:\, Y \sim 
  N\left(\alpha + X_{J_{1}} \beta_{J_{1}} + \cdots X_{J_k}\beta_{J_k}, 
  \;\sigma^2I\right).
\end{equation}
If the sequence $J_1,\ldots,J_d$ were deterministic instead of adaptive, then we might construct an ANOVA table for testing whether there was sufficient evidence to add each variable to the model in sequence. If the true mean of $Y$ is linear in $X_{J_1}, X_{J_3},$ and $X_{J_5}$, then $K = 5$ and $Y\in N_k$ for $k=6,\ldots,d$.

Once the algorithm for choosing the sequence $\bM(Y)$ has been specified, we will carry out tests for each comparison. The goal we have set for ourselves is meant to be analogous to sequential testing of ANOVA tables for a fixed sequence of models, though there certainly are other sequential model selection tasks that could be of interest.


\subsection{Stopping Rules}
For our purposes, a {\em stopping rule} $\hK(Y)$ is simply an estimate of the completion index $K$, which we could use to ultimately choose the model $M_{\hK}$ from the sequence.

For our purposes, a good stopping rule is one that does not go on refining the model unnecessarily once it is already adequate; e.g. we might want
\begin{equation}
  \P(\hK > K \gv \bM) \leqAS \alpha, \quad \text{ or } \quad \E\left(\frac{(\hK - K)\vee 1}{\hK \vee 1} \gv \bM \right) \leqAS \alpha
\end{equation}
Although it might initally seem more natural to require $\P(\hK < K \gv \bM) \leq \alpha$ (i.e., that we choose an adequate model with high probability) this goal is typically infeasible as stated. For example, suppose that all the coefficients in a linear regression model are $\beta_j=\ep>0$. As $\ep\to 0$, it becomes impossible to distinguish the true model from the global null model $\beta=0$.

In particular, we will focus on constructing a sequence of $p$-values $(p_1(Y),\ldots,p_d(Y))$. $p_k(Y)$ is a {\em conservative selective $p$-value} if
\begin{equation}
  \P_F\left(p_k(Y) \leq \alpha \gv \bM\right) \leqAS \alpha \text{ on } N_k.
\end{equation}
That is, conditional on the question asked, $p_k$ ought to be stochastically larger than uniform.

We say the sequence $(p_1,\ldots,p_d)$ are {\em jointly conservative selective $p$-values} if
\begin{equation}
  \P_F(p_k(Y) < \alpha_k, \forall k>K \gv \bM) \leqAS \prod_{k>K} \alpha_k, \quad \forall \alpha_1,\ldots, \alpha_d\in (0,1), F\in \cF
\end{equation}
Notice that $K$ is random but the conditional probability above still makes sense because $K$ is a function of $\bM$.

Joint conservatism of $p$-values is a minor generalization of requiring the null $p$-values to be mutually independent given the model sequence $\bM$.

\begin{proposition}
  Suppose that, given $\bM$, the $(p_k)_{k>K}$ are conditionally uniform and mutually independent. Then $(p_k)$ are jointly conservative.
\end{proposition}

A  benefit of having jointly conservative $p$-values is that we can apply the stopping rules of \citet{gsell2013sequential} to control error rates such as the model FDR and model FWER. The next section proposes a generic recipe for obtaining such $p$-values

\subsection{A Sequential Selection Variable for Pathwise FDR}

Define the {\em $k$th pathwise selection variable} as 
\begin{equation}
  S_k(Y) = (M_0,\ldots,M_k,p_1,\ldots,p_{k-1}).
\end{equation}
If for each $k$ the $p$-value $p_k$ is conservative conditional on $S_k$, then the $p$-value sequence $(p_k)$ is jointly conservative. 
\begin{proposition}
  Suppose that for each $k$, $p_k(Y)$ is conservative conditional on $S_k(Y)$ under $F\in M_k(Y)$. Then $(p_k)$ are jointly conservative given $\bM$.
\end{proposition}

Although conditioning on $S_k$ may appear daunting, in many cases it can be a simple matter, as we see next.

\section{Simplifications in Exponential Family Models}

\begin{model}[Exponential Family]\label{mod:exFam}
  Assume that we have finitely many sufficient statistics $T_j(Y)$, and $F$ is in an exponential family after adding enough sufficient statistics to the model:
  \begin{equation}
    Y \sim \exp\left\{\sum_j \theta_j ' T_j(y) - \psi(\theta)\right\} h(y)
  \end{equation}
  Assume we add sufficient statistics to the model one at a time in the order $T_{J_0}, T_{J_1}, \ldots$, so that model $k$ is
  \begin{equation}
    Y \sim \exp\left\{\theta_{J_0} ' T_{J_0}(y) + \theta_{J_1}'T_{J_1}(y) + \cdots + \theta_{J_k}'T_{J_k}(y) - \psi(\cdot)\right\} h(y)
  \end{equation}
\end{model}

Our running Example~\ref{ex:linReg} is a simple case of Model~\ref{mod:exFam}. In that case,
\begin{align*}
  \theta_0 = 1/2\sigma^2, &\qquad T_0(Y) = \|Y\|^2,\\
  \theta_j = \beta_j/\sigma^2, &\qquad 
T_j(Y) = X_j'Y, \text{ for } j>0
\end{align*}

Another example is that of the Wishart, in which we have
\begin{equation}
  Y \sim \exp\left\{ \sum_{ij} \theta_{ij}y_{ij} - \psi(\theta)\right\}h(y)
\end{equation}

To obtain a $p$-value for testing the null hypothesis $M_{k-1}$ against the alternative $M_k \setminus M_{k-1}$, we will typically condition on $U_k=(T_1,\ldots, T_{k-1})$ simply as a way to eliminate $\zeta_k=(\theta_1,\ldots, \theta_{k-1})$ from the problem. In many cases, $p_{k-1}$ is a function only of $U_k$. If so, it is not necessary to condition explicitly on the previous $p$-values.


\section{Simulation: Sequential Model Selection Using the Lasso Path}



\end{document}
