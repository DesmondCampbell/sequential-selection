\documentclass{article}


\usepackage[round]{natbib}
\usepackage{amsmath,amssymb,amsthm,bm,enumerate,mathrsfs,mathtools}
\usepackage{latexsym,color,verbatim,multirow}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage[margin=1.3in]{geometry}
\usetikzlibrary{shapes,arrows}
\tikzstyle{block} = [rectangle, draw, fill=white!20,
    text width=7em, text centered, rounded corners, minimum height=4em]
\tikzstyle{title} = [text width=7em, text centered, font=\bfseries]
\tikzstyle{line} = [draw, -latex']


\usepackage{mycommands}

\begin{document}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{model}[theorem]{Model}

\theoremstyle{definition}
\newtheorem{example}{Example}

\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\FDR}{\textnormal{FDR}}
\newcommand{\FCR}{\textnormal{FCR}}
\newcommand{\crt}{\phi}
\newcommand{\M}{\mathcal{M}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\Gv}{\;\;\big|\;\;}
%\newcommand{\cP}{\mathcal{P}}
\newcommand{\proj}{\cP}
\newcommand{\pow}{\text{Pow}}
\newcommand{\sF}{\mathscr{F}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\hJ}{\widehat{J}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\hK}{\widehat{K}}
\newcommand{\leqAS}{\overset{\textrm{a.s.}}{\leq}}


\newcommand*\mystrut{\vrule width0pt height0pt depth1.5ex\relax}
\newcommand{\underlabel}{\underbracket[1pt][.5pt]{\mystrut \quad\;\; \sub \quad\;\; }}
\newcommand{\JTcomment}[1]{{\color{blue}{(JT: \bf \sc #1) }}}
\newcommand{\WFcomment}[1]{{\color{red}{(WF: \bf \sc #1) }}}

\title{A Construction of Knockoffs for Generic Exponential Family Models}
\maketitle

\begin{abstract}
  We construct ``knockoff'' sufficient statistics having the
  pairwise exchangeability property of \citet{barber2014controlling}
\end{abstract}

\section{Goal}

Consider the linear model
\begin{equation}
  Y \sim \cN(X\beta, \sigma^2I_n)
\end{equation}
which is an exponential family with $d$ sufficient statistics $T=X'Y$ where $X\in \R^{n\times d}$ (although it is not really necessary, assume $\sigma^2$ is known).

The knockoff method of~\citet{barber2014controlling} constructs $\widetilde X\in \R^{n\times d}$ with the properties
\begin{equation}
  \widetilde X'\widetilde X = X'X, \quad\text{and}\quad \widetilde X'X = X'X.
\end{equation}
In effect this creates a parallel set of ``control'' sufficient statistics $U=\widetilde X'Y$. The key stochastic property of $(T,U)$ is the ``pairwise exchangeability'' property: namely, that its law is invariant to swapping $T_M$ with $U_M$ for a ``null'' subset $M\sub \{1,\ldots,d\}$ and $\beta_M=0$.

\section{Abstract Construction for Generic Exponential Families}

The ``simple'' algorithm is as follows. We begin with an exponential
family model, given below:
\begin{equation}
  \L_\theta(T) = e^{\theta't - \psi(\theta)}f^0(t).
\end{equation}
Assume without loss of generality that $f^0$ is a valid density. We introduce the following notation for the conditional laws (under the ``global null'' $\theta=0$):
\begin{equation}
  f_j^0(t_j \gv t_{-j}) = \frac{f^0(t)}{\int f^0(t_1,\ldots,t_{j-1},v,t_{j+1},\ldots,t_d) \,dv}
\end{equation}

To construct the first knockoff sufficient statistic $U_1$ (the ``control'' for $T_1$), we can generate a random variable distributed as
\begin{equation}
  \L(U_1 \gv T) = f_1^0(u_1 \gv t_{-1})
\end{equation}
So that (under $\theta=0$) we obtain distribution 
\begin{equation}
  \L_0(T,U_1) = f^1(t, u_1) = f^0(t)f_1^0(u_1\gv t_{-1})
\end{equation}
and more generally,
\begin{equation}
  \L_\theta(T,U_1) = e^{\theta't - \psi(\theta)}f^0(t)f_1^0(u_1\gv t_{-1}).
\end{equation}

Note that if $\theta_1=0$, the law of $(T,U_1)$ is invariant to swapping $T_1$ with its knockoff $U_1$, since
\begin{equation}
  f^1(t,u) = f_1^0(u \gv t_{-1})\; f_1^0(t_1 \gv t_{-1}) \int f^0(v,t_{-1})\,dv
\end{equation}

To construct the second knockoff $U_2$, we generate
\begin{equation}
  \L(U_2 \gv T, U_1) = f_2^1(u_2 \gv t_{-2}, u_1)
\end{equation}
where $f_2^1$ is the conditional density of $T_2$ given $T_{-2}$ and $U_1$. We can continue on in this manner,
\begin{equation}
  \L(U_j \gv T, U_{1,\ldots,j-1}) = f_{j-1}^j(u_j \gv t_{-j}, u_{1,\ldots,j-1}),
\end{equation}
finally obtaining
\begin{equation}\label{eq:finalKnockoff}
    \L_\theta(T,U) = e^{\theta't - \psi(\theta)}f^d(t,u)
\end{equation}

\begin{theorem}
  For $M \sub \{1,\ldots,d\}$ assume $\theta_M=0$. Then $\L_\theta(T,U)$ is invariant to swapping any subset $T_M$ with $U_M$.
\end{theorem}
\begin{proof}
First, note that this claim is true under the global null. In that case, we can first swap $T_1$ for $U_1$ if $1\in M$, then swap $T_2$ for $U_2$ if $2\in M$, and so on. At step $j$, it is clear from our construction that swapping or not swapping does not change the distribution of $(T,U_{1,\ldots,j-1})$. It also does not change the conditional distribution of $U_{j,\ldots,d}$: since $f^{j-1}$ is invariant to swapping arguments $t_k$ and $u_k$ for $k<j$, so must $f_j^{j-1}$ be.

Next consider the weaker condition $\theta_M=0$. Under the global null, we have shown in particular that the conditional distribution $\L_0(T_M,U_M \gv T_{-M},U_{-M})$ is invariant to swapping $T_M$ and $U_M$. But this conditional law only depends on $\theta_M$, so the conditional distribution is still invariant to swapping. Moreover, the marginal distribution of $(T_{-M},U_{-M})$ is obviously invariant to swapping $T_M$ with $U_M$; as a result, the joint distribution of $(T,U)$ is invariant.
\end{proof}
Note that, in the above proof, it was not really necessary that we actually construct conditionally independent knockoffs at each stage. It would be sufficient simply to construct knockoffs that are conditionally exchangeable with the true variable at each stage.

As a result, we can use our knockoffs $U$ just as \citet{barber2014controlling} did. For any choice of statistics $W(T,U)$ where $W_j\mapsto -W_j$ when we swap $T_j$ with $U_j$, we can plug $W$ into any the knockoff procedures they proposed.

\section{Linear Regression}

For $n\geq 2d$, assume we begin with $T = X'Y$ and let $Z\in\R^{n\times d}$ have orthogonal columns with $X'Z=0$. Then, we can carry out the program above by first choosing 
\begin{equation}
  \widetilde X_1 = \proj_{X_{-1}}X_1 + \|\proj_{X_{-1}}^\perp X_1\|Z_1,
\end{equation}
so that $\widetilde X_1'X_j = X_1'X_j$ for $j\neq 1$, and $\|\widetilde X_1\| = \|X_1\|$. Then, $U_1 = \widetilde X_1'Y$ is a knockoff statistic for $T_1=X_1'Y$.

In the second step, we similarly take 
\begin{equation}
  \widetilde X_2 = \proj_{[X_{-1},U_1]}X_2 + \|\proj_{[X_{-1},U_1]}^\perp X_2\|Z_2
\end{equation}
so that $\widetilde X_2'X_j = X_2'X_j$ for $j\neq 2$, and $\widetilde X_1'\widetilde X_2 = \widetilde X_1'X_2 = X_1'X_2$. $U_2 = \widetilde X_2'Y$ is also a knockoff statistic for $T_2=X_2'Y$. We can continue in the obvious way to construct knockoffs $U_3,\ldots,U_d$.

\section{Poisson Process}

Next consider a simple non-Gaussian example, a Poisson process with sufficient statistics corresponding to overlapping windows. Suppose that $N$ is a Poisson process on $[0,300]$ with intensity equal to $e^{\lambda_1 1\{x<200\} + \lambda_2 1\{x>100\}}$. The sufficient statistics are:
\begin{equation}
  T_1=N([0,200]), \quad T_2=N([100,300]).
\end{equation}
Can we construct knockoffs $U_1,U_2$?

First, consider trying to construct conditionally independent knockoffs. We must construct $U_1$, which (conditionally on $T_2$) is independent of $T_1$ and has its null distribution. In particular, we can construct an artifical process $N^1$ which has $T_2$ points uniformly distributed on $[100,300]$, and is a Poisson process with unit intensity on $[0,100]$, then take $U_1=N^1([0,200])$.

Next, consider constructing $U_2$. Marginally under $\theta=0$, $N$ and $N^1$ are two Poisson processes of unit intensity, conditioned on having the same count falling into $[100,300]$. Then we have to condition further on the values of $N([0,200])$ and $N^1([0,200])$. This does not appear to be a trivial problem...

This is one case where it might be easier not to insist on conditional independence. For example, we could artificially generate $N$ on $[300,400]$ with unit intensity. We can then take $U_1=N([200,400])$ ($T_1$ and $U_1$ are conditionally exchangeable if $\theta_1=0$), and $U_2=N([0,100] \cup [300,400])$ (exchangeable with $T_2=N([100,300])$ given $T_1$ and $U_1$, if $\theta_2=0$).

\bibliographystyle{plainnat}
\bibliography{biblio}

\end{document}
