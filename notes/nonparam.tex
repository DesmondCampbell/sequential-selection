\documentclass{article}


\usepackage[round]{natbib}
\usepackage{amsmath,amssymb,amsthm,bm,enumerate,mathrsfs,mathtools}
\usepackage{latexsym,color,verbatim,multirow}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\newcommand{\real}{\mathbb{R}}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\begin{document}

\title{Nonparametric selective bootstrap}

\maketitle

\section{Non-bootstrap world}

Suppose we have a model selection procedure $\hat{M}$ acting on sufficient
statistics $Z$ choosing among a (finite) set of models. So, our selected model
is $M=\hat{M}(Z)$. Our selected model suggests a hypothesis of interest that we want to test.

We will apply $\hat{M}$ on data of the form
$$
\hat{Z}_n = \frac{1}{n} \sum_{i=1}^n Z(t_i) 
$$
where $t_i \overset{\text{IID}}{\sim} F$ where $F$
is some distribution on some sample space ${\cal X}$. In the pairs bootstrap for 
regression context, we should think of $t_i=(y_i, X_i)$ and $F$ 
is a distribution on $\real \times \real^p$ and, if we use
forward stepwise our
selection is a function of 
$$\frac{1}{n}(X^Ty, X^TX)$$
and our $Z$ should be taken to be
$$
Z(t_i) = (y_iX_i, X_iX_i^T).
$$

Note that $Z$ is a statistical functional that we might write as
$$
Z(F) = E_{F}(Z(t))
$$
and what we really observe $\hat{Z}_n = Z(\hat{F}_n)$. 

We want to carry out a selective test of
$$
H_0: \eta_{M}^T \gamma_M(F) = 0
$$
where $\eta_M \in \real^{p \times k}$ and $\gamma_M$ is some statistical
function possibly different from $Z$. In principle it can depend
on the model we have selected, but we will drop the $M$ from now on.
That is, we are now looking at
$$
\gamma_M(\hat{F}_n) = \sum_{i=1}^n \gamma(t_i)
$$
where $t_i \overset{IID}{\sim} F$.

We assume that $Z$ is measurable with respect to $\gamma$, i.e.
$\gamma$ includes $Z$ as part of its coordinates or something. This means
that it makes sense to talk about $\hat{M}$ evaluated
on a sample of $\gamma$'s.

\subsection{One sample problem, mean or median?}

Here is an ``example.''
Suppose we have a one-sample problem and if the empirical skewness
is larger than some threshold, we test whether the median of the 
distribution, otherwise we report the mean. In one case,
$\gamma_M$ would compute sample skewness and sample median, in the other
it would compute the sample skewness and sample mean. When choosing to test the sample mean, I suppose the model is all distributions on $\real$ for
which the sample mean and sample skewness and sample mean 
are asymptotically Gaussian. When
reporting the sample median, the model is all distributions on $\real$
for which the sample skewness and sample median are asymptotically jointly Gaussian.

\subsection{Hypothesis tests under unselective distribution}


For large enough $n$, under our model $M$ suppose that
$$
\gamma(\hat{F}_n) \approx N(\gamma(F), n^{-1} \Sigma(F))
$$
where
$$
\Sigma(F) = E_{F}\left((\gamma(t) - \gamma(F))(\gamma(t) - \gamma(F))^T \right).
$$

{\bf Note: as the functional
we choose may depend on $M$, the normal approximation generally depends 
on the selected model -- I am not always going to emphasize that here.}

If this is our reference distribution, we should base our test on the 
conditional law 
$$n^{1/2}\Sigma(F)^{-1}\gamma(\hat{F}_n) \big \vert n^{1/2} P_{\eta}^{\perp}\Sigma(F)^{-1}\gamma(\hat{F}_n).$$ 
where $P_{\eta}=\eta\eta^{\dagger}$ and $P_{\eta}^{\perp} = I - P_{\eta}$.
This is because
we know that, under $H_0:\eta^T\gamma(F)=0$
this distribution does not depend on $P_{\eta}^{\perp}\gamma(F)$ which
is everything except what we wanted to test. 
 
This 
law has covariance $n^{-1} \Theta(\eta, \Sigma(F))$ where
$$
\Theta(\eta, \Sigma(F)) = \left(\Sigma(F)^{-1} - \Sigma(F)^{-1} P_{\eta}^{\perp} \left(P_{\eta}^{\perp} \Sigma(F)^{-1} P_{\eta}^{\perp} \right)^{\dagger}  P_{\eta}^{\perp} \Sigma(F)^{-1}  \right)
$$
which is an $O(1)$ covariance matrix.
Under $H_0$, this law has mean 0. Under any alternative, its mean grows like $n^{1/2}$.

Under $H_0$
this distribution can be realized by 
$$
L(\eta, \Sigma(F), W) = n^{1/2} \left(\Sigma(F)^{-1} W - \Sigma(F)^{-1} P_{\eta}^{\perp} \left(P_{\eta}^{\perp} 
\Sigma(F)^{-1} P_{\eta}^{\perp} \right)^{\dagger}  P_{\eta}^{\perp} W \right)
$$
where $W \sim N(\mu, n^{-1}\Sigma(F))$, is any normal distribution with
$\eta^T\mu=0$.

In other words, if $W \sim N(\mu, n^{-1}\Sigma(F))$ 
then under $H_0: \eta^T\gamma(F)=0$ we have, asymptotically
$$
 L(\eta, \Sigma(F), \gamma(\hat{F}_n))  \overset{n \rightarrow \infty}{\Rightarrow} L(\eta, \Sigma(F), W) = N(0,  \Theta(\eta, \Sigma(F))).
$$


Therefore, if we can
sample from this distribution, we know how to
do an asymptotic unselective test of $H_0:\eta_M^T\gamma(F)=0$ using the 
normal approximation.


\subsection{Selective test}
\label{sec:selective}

Recall that we have chosen $\eta_M$ based on the outcome $\hat{M}(\hat{Z}_n)$. Following the general setup of the
selected model framework, we just have to design
a test for a fixed $M$.

The parametric selective test is based on the distribution of 
$$
L(\eta, \Sigma(F), \gamma(\hat{F}_n)) \big \vert \hat{M}(\gamma(\hat{F}_n)) = M.
$$
To emphasize that this depends on the selected model, we might write
$$
L_M(\eta_M, \Sigma_M(F), \gamma_M(\hat{F}_n)) \big \vert \hat{M}(\gamma_M(\hat{F}_n)) = M.
$$

Now, if $W \sim N(\mu, n^{-1}\Sigma(F))$ is any normal distribution
such that $\eta^T\mu=0$ then by familiar exponential family principles, the above law
is equal to the law of
$$
L_M(\eta_M, n^{-1}\Sigma(F), W) \big \vert \hat{M}(W) = M.
$$

{\bf This means it is enough to find any normal distribution
$W \sim N(\mu, n^{-1}\Sigma(F))$ such that $\eta^T\mu=0$ to carry out the
selective test.
}

\section{Bootstrap world}

We apply non-parametric bootstrap to the pairs 
yielding samples
$$
t^{*,b} \in {\cal X}^n, \qquad 1 \leq  b \leq B
$$
and  bootstrapped sufficient statistics
$$
\gamma^{*,b} = \frac{1}{n} \sum_{i=1}^n \gamma(t_i^{*,b}),  \qquad 1 \leq  b \leq B
$$

These are of course just evaluating $\gamma$ on 
IID draws of size $n$ from $\hat{F}_n$. So,
we can write
$\gamma^{*,b} = \gamma(\hat{F}_n^{*,b})$
where $\hat{F}_n^{*,b}$ is the empirical
distribution function of sampling with replacement from the original data.

For large enough $n$, the law of $\gamma(\hat{F}_n^{*,b})$ is going to be converging
to the Gaussian 
$$
\gamma(\hat{F}_n^{*,b}) | \hat{F}_n \sim N(\gamma(\hat{F}_n), n^{-1} \Sigma(\hat{F}_n))
$$
where
$$
\Sigma(\hat{F}_n) = \frac{1}{n} \sum_{i=1}^n \left(\gamma(t_i) - \gamma(\hat{F}_n) \right)\left(\gamma(t_i) - \gamma(\hat{F}_n) \right)^T.
$$


Therefore, our unselective bootstrap should be giving samples from
roughly $ N(\gamma(\hat{F}_n), n^{-1} \Sigma(\hat{F}_n))$. That is,
taking $B$ very large:
$$
\frac{1}{B}\sum_{b=1}^B \eta^T \gamma(\hat{F}^{*,b}_n) \approx \eta^T\gamma(\hat{F}_n).
$$
This suggests we correct the unselective bootstrap by drawing
$$
\gamma(\hat{F}^{*,b}_n) - P_{\eta}\gamma(\hat{F}_n) | \hat{F}_n \approx N(P_{\eta}^{\perp}\gamma(\hat{F}_n) ,n^{-1} \Sigma(\hat{F}_n)).
$$
With this correction, we see that
$$
L(\eta, \Sigma(\hat{F}_n), \gamma(\hat{F}^{*,b}_n) - P_{\eta} \gamma(\hat{F}_n)) \big \vert \hat{F}_n
 \approx N(0 ,n^{-1} \Theta(\eta, \Sigma(\hat{F}_n))).
$$
As $n$ gets larger, $\Sigma(\hat{F}_n)$ will approach $\Sigma(F)$. 
If the RHS above were actually $$N(0 ,n^{-1} \Theta(\eta, \Sigma(F))),$$ then
by our earlier observation at the end of Section \ref{sec:selective} this is sufficient to carry out the selective test exactly.
However, the variance is not exactly correct. In general,
plugging in consistent estimates of variance is OK in selective inference (not sure
if Xiaoying and I put it in the first paper on asymptotics, but she has given me a proof and definitely in the
randomized paper we will have a concrete statement about this). In Rob's example of $n=30$, the sample size might be too small still, but asymptotically
I think it will be fine. Maybe a degrees of freedom correction will help.

The final algorithm is:
we draw nonparametric bootstrap samples
$$
\gamma(\hat{F}^{*,b}_n) - P_{\eta}\gamma(\hat{F}_n)
$$
keeping only those for which $\hat{M}(\gamma(\hat{F}^{*,b}_n) - P_{\eta}\gamma(\hat{F}_n)) = \hat{M}(\gamma(\hat{F}_n))$.

For those that survive, we compute
$$
L(\eta, \Sigma(\hat{F}_n), \gamma(\hat{F}^{*,b}_n) - P_{\eta}\gamma(\hat{F}_n))
$$
and take any test statistic of our choosing.

\section{No nuisance parameters}

When there are no nuisance parameters (i.e. $\eta=I$ or we are make other assumptions about $Z(F)$), we do not need the
function $L$. This is the example I tried this morning. In this
case, $\gamma=Z$ and we just draw nonparametric bootstrap samples
$$
Z(\hat{F}^{*,b}_n) - P_{\eta}Z(\hat{F}_n)
$$
keeping the ones that satisfy selection and compute any test statistic of our 
choosing on these bootstrap samples. 

\end{document}


